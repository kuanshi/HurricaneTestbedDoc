<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3.2. Asset Description &mdash; Regional Resilience Determination Tool  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sphinxcontrib-images\LightBox2\lightbox2\dist\css\lightbox.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinxcontrib-images\LightBox2\lightbox2\dist\js\lightbox-plus-jquery.min.js"></script>
        <script src="../../../_static/sphinxcontrib-images\LightBox2\lightbox2-customize\jquery-noconflict.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="3.3. Hazard Characterization" href="hazard_characterization.html" />
    <link rel="prev" title="3.1. Overview" href="overview.html" />
<script src="https://cdn.jsdelivr.net/npm/vega@5.12.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@4.13.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6.8.0"></script>
<style media="screen">.vega-actions a {margin-right: 5px;}</style>
<link href="../../../_static/css/bootstrap.css" rel="stylesheet">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #F2F2F2" >
<a href="https://simcenter.designsafe-ci.org/" style="margin-bottom: 0px;">
  <img src="../../../_static/img/SimCenter-Only.png" class="logo" alt="Org-Logo" />
</a>
<hr style="margin: 0px;">

  <a href="../../../index.html">



  
  <img src="../../../_static/R2D-Logo.png" class="logo" alt="Logo"/>

</a>


  
  



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/about/R2D/about.html">1. About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/ack.html">2. Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/license.html">3. Copyright and License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/glossary.html">4. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/abbreviations.html">5. Abbreviations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/usage/desktop/releasenotes.html">6. Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/installation/desktop/installation.html">1. Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/usage/desktop/usage.html">2. User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/troubleshooting/desktop/troubleshooting.html">3. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/examples/desktop/examples.html">4. Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reqments/R2D.html">5. R2D Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/bugs.html">6. Bugs &amp; Feature Requests</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Testbeds</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../sf_bay_area/index.html">1. San Francisco, CA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../atlantic_city/index.html">2. Atlantic County, NJ</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. Lake Charles, LA</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="overview.html">3.1. Overview</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.2. Asset Description</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#phase-i-attribute-definition">3.2.1. Phase I: Attribute Definition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#phase-ii-inventory-generation">3.2.2. Phase II: Inventory Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ai-ml-techniques-combined-with-computer-vision">AI/ML Techniques combined with Computer Vision</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#phase-iii-augmentation-using-third-party-data-site-specific-observations-and-existing-knowledge">3.2.3. Phase III: Augmentation Using Third-Party Data, Site-specific Observations, and Existing Knowledge</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#attribute-dws-ii">Attribute: DWS II</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute-lulc">Attribute: LULC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute-yearbuilt">Attribute: YearBuilt</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute-garage">Attribute: Garage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute-buildingtype">Attribute: BuildingType</a></li>
<li class="toctree-l4"><a class="reference internal" href="#attribute-avgjantemp">Attribute: AvgJanTemp</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#populated-inventories">3.2.4. Populated Inventories</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hazard_characterization.html">3.3. Hazard Characterization</a></li>
<li class="toctree-l2"><a class="reference internal" href="asset_representation.html">3.4. Asset Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="response_simulation.html">3.5. Response Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="damage_and_loss.html">3.6. Damage and Loss Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="uncertainty_quantification.html">3.7. Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_outputs.html">3.8. Example Outputs</a></li>
<li class="toctree-l2"><a class="reference internal" href="sample_results.html">3.9. Verification Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="validation_results.html">3.10. Validation Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="future_refinements.html">3.11. Opportunity Areas for Future Refinements</a></li>
<li class="toctree-l2"><a class="reference internal" href="best_practices.html">3.12. Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="feedback_request.html">3.13. Feedback</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technical Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../technical_manual/desktop/technical_manual.html">1. Technical Manual</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/how_to_build/desktop/how_to_build.html">1. How to Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/architecture/desktop/architecture.html">2. Software Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/how_to_extend/desktop/how_to_extend.html">3. How to Extend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/verification/desktop/verification.html">4. Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/coding_style/desktop/coding_style.html">5. Coding Style</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/examples/desktop/examples.html">6. Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #F2F2F2" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Regional Resilience Determination Tool</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html"><span class="section-number">3. </span>Lake Charles, LA</a> &raquo;</li>
      <li><span class="section-number">3.2. </span>Asset Description</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/common/testbeds/lake_charles/asset_description.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="asset-description">
<span id="lbl-testbed-lc-asset-description"></span><h1><span class="section-number">3.2. </span>Asset Description<a class="headerlink" href="#asset-description" title="Permalink to this headline"></a></h1>
<p>This section describes how a large-scale building inventory was constructed in two phases. The initial
phase of this work involved identifying the attributes needed. The second phase involved work to
obtain these attributes for each building using machine learning and
techniques from computer vision to create an initial set of attributes. The remaining attributes were
then obtained using other data sources as discussed below.</p>
<div class="section" id="phase-i-attribute-definition">
<h2><span class="section-number">3.2.1. </span>Phase I: Attribute Definition<a class="headerlink" href="#phase-i-attribute-definition" title="Permalink to this headline"></a></h2>
<p>All the attributes required for loss estimation were first identified to develop the Building Inventory
data model. This Building Inventory data model presented in <a class="reference internal" href="#tab-bldg-inv-data-model-lc"><span class="std std-numref">Table 3.2.1.1</span></a>
provides a set of attributes that will be assigned to each asset to form the building inventory file
serving as input to the workflow. For each attribute a
row in the table is provided. Each row has a number of columns: the attribute name, description,
format (alphanumeric, floating point number, etc.), the data source used to define that attribute.
An expanded version of <a class="reference internal" href="#tab-bldg-inv-data-model-lc"><span class="std std-numref">Table 3.2.1.1</span></a> with the full details of this data
model are available on <a class="reference external" href="https://www.designsafe-ci.org/data/browser/public/designsafe.storage.published//PRJ-3207v3/01.%20Input:%20BIM%20-%20Building%20Inventory%20Data">DesignSafe PRJ-3207</a>.</p>
<table class="colwidths-given docutils align-center" id="tab-bldg-inv-data-model-lc">
<caption><span class="caption-number">Table 3.2.1.1 </span><span class="caption-text">Building inventory data model, detailed for Lake Charles Inventory.</span><a class="headerlink" href="#tab-bldg-inv-data-model-lc" title="Permalink to this table"></a></caption>
<colgroup>
<col style="width: 15%" />
<col style="width: 40%" />
<col style="width: 25%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Description Attribute</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Format</p></th>
<th class="head"><p>Source</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>ID</strong></p></td>
<td><p>Building unique ID.</p></td>
<td></td>
<td><p>User defined</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Latitude</strong></p></td>
<td><p>Latitude of the Building Centroid (inside polygon).</p></td>
<td><p>Floating point number (Decimal Degrees)</p></td>
<td><p><a class="reference external" href="https://github.com/microsoft/USBuildingFootprints">Microsoft Maps</a></p></td>
</tr>
<tr class="row-even"><td><p><strong>Longitude</strong></p></td>
<td><p>Longitude of the Building Centroid (inside polygon).</p></td>
<td><p>Floating point number (Decimal Degrees)</p></td>
<td><p><a class="reference external" href="https://github.com/microsoft/USBuildingFootprints">Microsoft Maps</a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>OccupancyClass</strong></p></td>
<td><p>Subclassifications of buildings across various categories of Residential (RES), Commercial (COM).</p></td>
<td><p>Choices: RES1, RES3, COM1</p></td>
<td><p>StreetView</p></td>
</tr>
<tr class="row-even"><td><p><strong>BuildingType</strong></p></td>
<td><p>Core construction material type</p></td>
<td><p>Choices: Wood</p></td>
<td><p>Assume all in wood</p></td>
</tr>
<tr class="row-odd"><td><p><strong>YearBuilt</strong></p></td>
<td><p>Year of Construction</p></td>
<td><p>Integer</p></td>
<td><p>StreetView</p></td>
</tr>
<tr class="row-even"><td><p><strong>NumberOfStories</strong></p></td>
<td><p>Number of stories estimated via image processing</p></td>
<td><p>Integer</p></td>
<td><p>StreetView</p></td>
</tr>
<tr class="row-odd"><td><p><strong>DWSII</strong></p></td>
<td><p>DesignWindSpeed II in mph</p></td>
<td><p>Floating point number</p></td>
<td><p>ATC API (ASCE 7)</p></td>
</tr>
<tr class="row-even"><td><p><strong>AvgJanTemp</strong></p></td>
<td><p>Average temperature in January below or above critial value of 25F.</p></td>
<td><p>Choices: Above, Below</p></td>
<td><p>User specified</p></td>
</tr>
<tr class="row-odd"><td><p><strong>RoofShape</strong></p></td>
<td><p>Roof classified into equivalent hip, gable or flat</p></td>
<td><p>Choices: Hip, Gable, Flat</p></td>
<td><p>Aerial Imagery</p></td>
</tr>
<tr class="row-even"><td><p><strong>RoofSlope</strong></p></td>
<td><p>Slope of roof (ratio of rise/vertical over run/horizontal dimensions) covering the majority of the dwelling.</p></td>
<td><p>Floating point number</p></td>
<td><p>Aerial + StreetView Imagery</p></td>
</tr>
<tr class="row-odd"><td><p><strong>MeanRoofHt</strong></p></td>
<td><p>Mean height of roof system in ft</p></td>
<td><p>Floating point number</p></td>
<td><p>Aerial + StreetView Imagery</p></td>
</tr>
<tr class="row-even"><td><p><strong>Garage</strong></p></td>
<td><p>Assessor-provided type of garage.</p></td>
<td><p>Choices: 0, 1</p></td>
<td><p>StreetView</p></td>
</tr>
<tr class="row-odd"><td><p><strong>LULC</strong></p></td>
<td><p>Land Use Land Cover class</p></td>
<td><p><a class="reference external" href="https://github.com/NHERI-SimCenter/SimCenterDocumentation/blob/master/docs/common/testbeds/lake_charles/data/LULC.json">LULC conversion</a></p></td>
<td><p><a class="reference external" href="http://www.webgis.com/terr_pages/LA/lulcutm/calcasieu.html">WebGIS</a></p></td>
</tr>
<tr class="row-even"><td><p><strong>PlanArea</strong></p></td>
<td><p>Plan area (optional)</p></td>
<td><p>Floating point number (optional)</p></td>
<td><p>User defined or estimated from polygon</p></td>
</tr>
<tr class="row-odd"><td><p><strong>AnalysisDefault</strong></p></td>
<td><p>Defines the default level of fidelity for analysis</p></td>
<td><p>Choices: 1, 2, 3 (optional)</p></td>
<td><p>User defined</p></td>
</tr>
<tr class="row-even"><td><p><strong>WindowArea</strong></p></td>
<td><p>Window area ratio</p></td>
<td><p>Floating point number (optional, default 0.2)</p></td>
<td><p>User defined</p></td>
</tr>
<tr class="row-odd"><td><p><strong>WindZone</strong></p></td>
<td><p>Rating of the amount of wind pressure a manufactured home</p></td>
<td><p>Choice: I, II, III, IV (optional, default I)</p></td>
<td><p>User defined</p></td>
</tr>
<tr class="row-even"><td><p><strong>RoofSystem</strong></p></td>
<td><p>Roof system</p></td>
<td><p>Choices: Wood, OWSJ (optional, default Wood)</p></td>
<td><p>User defined</p></td>
</tr>
<tr class="row-odd"><td><p><strong>SheathingThickness</strong></p></td>
<td><p>Thickness of sheathing (inch)</p></td>
<td><p>Floating point number (optional, default 1.0)</p></td>
<td><p>User defined</p></td>
</tr>
<tr class="row-even"><td><p><strong>FloodZone</strong></p></td>
<td><p>FEMA FloodZone designation as defined by Flood Insurance Rate Maps</p></td>
<td><p><a class="reference external" href="https://snmapmod.snco.us/fmm/document/fema-flood-zone-definitions.pdf">FEMA designation</a> (optional, default X)</p></td>
<td><p>User specified</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Representation Attribute</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Format</strong></p></td>
<td><p><strong>Source</strong></p></td>
</tr>
<tr class="row-even"><td><p><strong>HazusClassW</strong></p></td>
<td><p>Hazus building classes as defined for wind hazards</p></td>
<td><p>Choices: WSF1-2, WMUH1-3</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>HazardProneRegion</strong></p></td>
<td><p>Hazard Prone Regions for Hazus wind vulnerability assignments for WSF1-2</p></td>
<td><p>Choices: yes, no</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-even"><td><p><strong>WindBorneDebris</strong></p></td>
<td><p>Wind Borne Debris for Hazus wind vulnerability assignments for WSF1-2</p></td>
<td><p>Choices: yes, no</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>SecondaryWaterResistance</strong></p></td>
<td><p>Secondary Water Resistance for Hazus wind vulnerability assignments for WSF1-2, WMUH1-3</p></td>
<td><p>Choices: yes, no</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-even"><td><p><strong>RoofCover</strong></p></td>
<td><p>Roof cover for Hazus wind vulnerability assignments for WMUH1-3</p></td>
<td><p>Choices: N/A, BUR, SPM</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>RoofQuality</strong></p></td>
<td><p>Roof cover quality for Hazus wind vulnerability assignments for WMUH1-3</p></td>
<td><p>Choices: N/A, poor, good</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-even"><td><p><strong>RoofDeckAttachmentW</strong></p></td>
<td><p>Roof Deck Attachment for wood for Hazus wind vulnerability assignments for WSF1-2, WMUH1-3</p></td>
<td><p>Choices: A, B, C, D</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>RoofToWallConnection</strong></p></td>
<td><p>Roof to Wall Connection for Hazus wind vulnerability assignments for WSF1-2, WMUH1-3</p></td>
<td><p>Choices: strap, toe-nail</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Shutters</strong></p></td>
<td><p>Window opening protection for Hazus wind vulnerability assignments for WSF1-2, WMUH1-3, MMUH1-3</p></td>
<td><p>Choices: yes, no</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>AugmentedGarage</strong></p></td>
<td><p>Attached garage for Hazus wind vulnerability assignments for WSF1-2</p></td>
<td><p>Choices: none, SFBC 1994, standard, weak</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
<tr class="row-even"><td><p><strong>TerrainRoughness</strong></p></td>
<td><p>HAZUS-defined terrain classification (x100) based on LULC data</p></td>
<td><p>Choices: 3, 15, 35, 70, 100</p></td>
<td><p>Rulesets (see <a class="reference internal" href="asset_representation.html#lbl-testbed-lc-asset-representation"><span class="std std-ref">Asset Representation</span></a>)</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="phase-ii-inventory-generation">
<h2><span class="section-number">3.2.2. </span>Phase II: Inventory Generation<a class="headerlink" href="#phase-ii-inventory-generation" title="Permalink to this headline"></a></h2>
<p>This section describes how the large-scale building inventory was constructed for Lake Charles using
a phased approach that used machine learning, computer vision algorithm and data distributions to
generate all attributes required for
the corresponding loss assessment. It is emphasized that the intent is to demonstrate how an
inventory could be constructed and not to address potential errors, omissions or inaccuracies in
the source data, i.e., source data are assumed to be accurate and no additional quality assurance
was conducted outside of addressing glaring omissions or errors.</p>
<p>For each of the attributes identified in <a class="reference internal" href="#tab-bldg-inv-data-model-lc"><span class="std std-numref">Table 3.2.1.1</span></a>,
a description of the attribute and information on how the data was identified and validated is presented.</p>
<div class="section" id="ai-ml-techniques-combined-with-computer-vision">
<h3>AI/ML Techniques combined with Computer Vision<a class="headerlink" href="#ai-ml-techniques-combined-with-computer-vision" title="Permalink to this headline"></a></h3>
<p>Many of those attributes were generated with the SimCenter’s
<a class="reference external" href="https://nheri-simcenter.github.io/BRAILS-Documentation/index.html">BRAILS</a> CityBuilder application.
To avoid replication in the text, the following describes how those attributes
were obtained. The sections below will present the validation of the data.</p>
<p>CityBuilder is a python application that incorporates different AI/ML modules from BRAILS for performing
specific tasks. The user creates a python script importing City Builder, constructing a CityBuilder
object and then asking that object to build the inventory. The example script shown below, albeit
our GoogleMapAPIKey removed, will create a building inventory file for use in the workflow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the module from BRAILS</span>
<span class="kn">from</span> <span class="nn">brails.CityBuilder</span> <span class="kn">import</span> <span class="n">CityBuilder</span>
<span class="c1"># Initialize the CityBuilder</span>
<span class="n">cityBuilder</span> <span class="o">=</span> <span class="n">CityBuilder</span><span class="p">(</span><span class="n">attributes</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;occupancy&#39;</span><span class="p">,</span><span class="s1">&#39;roofshape&#39;</span><span class="p">],</span>
                  <span class="n">numBldg</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">random</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">place</span><span class="o">=</span><span class="s1">&#39;Lake Charles, Louisiana&#39;</span><span class="p">,</span>
                  <span class="n">GoogleMapAPIKey</span><span class="o">=</span><span class="s1">&#39;REMOVED GOOGLE API KEY&#39;</span><span class="p">)</span>
<span class="c1"># create the city-scale BIM file</span>
<span class="n">BIM</span> <span class="o">=</span> <span class="n">cityBuilder</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
</pre></div>
</div>
<p>Upon execution, CityBuilder will:</p>
<ol class="arabic simple">
<li><p>Download footprints for all buildings in the interested region, Lake Charles, from the
<a class="reference external" href="https://github.com/microsoft/USBuildingFootprints">Microsoft Footprint Dataset</a> (<a class="reference internal" href="#microsoft18" id="id2"><span>[Microsoft18]</span></a>).</p></li>
<li><p>Calculates the coordinate (Latitude, Longitude) for each building’s centroid, based on the footprint information.</p></li>
<li><p>Download both a satellite image and a street view image for each building using Google API’s and the extracted coordinates.</p></li>
<li><p>Perform computations on the images to obtain the interested building attributes using a series of pre-trained AI models.</p></li>
</ol>
<div class="section" id="attribute-roofshape">
<h4>Attribute: RoofShape<a class="headerlink" href="#attribute-roofshape" title="Permalink to this headline"></a></h4>
<p>The RoofShape is obtained by CityBuilder using the BRAILS Roof shape module. The roof shape module
determines roof shape based on a satellite image obtained for the building. The module uses machine
learning, specifically it utilizes a convolutional neural network that has been trained on satellite
images. In AI/ML terminology the Roof Shape module is an image classifier: it takes an image and
classifies it into one of three categories used in HAZUS: gable, hip, or flat as shown in
<a class="reference internal" href="#roof-shape"><span class="std std-numref">Fig. 3.2.2.1</span></a>. The original training of the AI model utilized 6,000 images obtained from google
satellite imagery in conjunction with roof labels obtained from
<a class="reference external" href="https://www.openstreetmap.org/">Open Street Maps</a>. As many roofs have more complex shapes, a
similitude measure is used to determine which of these roof geometries is the best match to a given roof.
More details of the classifier can be found
<a class="reference external" href="https://nheri-simcenter.github.io/BRAILS-Documentation/common/user_manual/modules/roofClassifier.html">here</a>.
The trained classifier was employed here to classify the roof information for Lake Charles.</p>
<div class="align-center figure" id="roof-shape">
<a class="reference internal image-reference" href="../../../_images/RoofShape.png"><img alt="../../../_images/RoofShape.png" src="../../../_images/RoofShape.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.1 </span><span class="caption-text">Roof type classification with examples of aerial images (a-f) and simplified archetypes (d-f) used by Hazus.</span><a class="headerlink" href="#roof-shape" title="Permalink to this image"></a></p>
</div>
<p>The performance of the roof shape classifier was validated against two ground truth datasets.
The first is comprised of 125 manually labeled satellite images sampled from OpenStreetMap from
across the US, retaining only those with unobstructed views of building roofs (a cleaned dataset).
The second is 56 residences assessed by StEER for which roof types were one of the three HAZUS classes,
e.g., removing all roofs labeled as “Complex” according to StEER’s distinct image labeling standards.
The validation process is documented
<a class="reference external" href="https://nheri-simcenter.github.io/BRAILS-Documentation/common/technical_manual/roof.html">here</a>.
The confusion matrices are presented in <a class="reference internal" href="#roof-shape-vali"><span class="std std-numref">Fig. 3.2.2.2</span></a>. These matrices visually present
the comparison between the predictions and actual data and should have values of 1.0 along the diagonal
if the classification is perfect, affirming the accuracy of the classification by the roof shape classifier.</p>
<div class="align-center figure" id="roof-shape-vali">
<a class="reference internal image-reference" href="../../../_images/RoofShapeVali.png"><img alt="../../../_images/RoofShapeVali.png" src="../../../_images/RoofShapeVali.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.2 </span><span class="caption-text">Validation of BRAILS predicted roof shapes to roof shapes from OpenStreetMap and StEER.</span><a class="headerlink" href="#roof-shape-vali" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="attribute-occupancyclass">
<h4>Attribute: OccupancyClass<a class="headerlink" href="#attribute-occupancyclass" title="Permalink to this headline"></a></h4>
<p>The occupancy class attribute is also determined by CityBuilder using the occupancy class classifier
module in BRAILS. The occupancy classifier is also a convolutional neural network. This network trained
using 15,743 google street view images with labels derived from OpenStreetMaps and the NJDEP dataset in
the <a class="reference external" href="https://nheri-simcenter.github.io/R2D-Documentation/common/testbeds/atlantic_city/asset_description.html">Atlantic County, NJ testbed Asset Description</a>.
This classifier labels buildings as one of: RES1 (single family building), RES3
(multi-family building), COM1 (Commercial building). More details of the classifier can be found
<a class="reference external" href="https://nheri-simcenter.github.io/BRAILS-Documentation/common/user_manual/modules/occupancyClassifier.html">here</a>.</p>
<p>The performance of the classifier was validated against a ground truth dataset that contains 293 street
view images from the United States with unobstructed views of the buildings (cleaned data). The full
validation was documented <a class="reference external" href="https://nheri-simcenter.github.io/BRAILS-Documentation/common/technical_manual/occupancy.html">here</a>.
The confusion matrix, which presents visually the predictions versus actual data from the original
293 image validation set, is as shown in <a class="reference internal" href="#occ-class-vali"><span class="std std-numref">Fig. 3.2.2.3</span></a> for OpenStreetMaps (see plot a), and
the NJDEP dataset (see plot b). <a class="reference internal" href="#occ-class-pred"><span class="std std-numref">Fig. 3.2.2.4</span></a> displays the BRAILS occupancy predictions for
Lake Charles for a selected region. Note that only those classified as RES1 or RES3 are retained in
this testbed focused on residential construction (and the COM1 is assigned to the buildings that are classified
other than the two residential classes by BRAILS).</p>
<div class="align-center figure" id="occ-class-vali">
<a class="reference internal image-reference" href="../../../_images/OccupancyClassVali.png"><img alt="../../../_images/OccupancyClassVali.png" src="../../../_images/OccupancyClassVali.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.3 </span><span class="caption-text">Validation of BRAILS predicted occupancy classes to OpenStreetMap and NJDEP.</span><a class="headerlink" href="#occ-class-vali" title="Permalink to this image"></a></p>
</div>
<div class="align-center figure" id="occ-class-pred">
<a class="reference internal image-reference" href="../../../_images/OccupancyClassPred.png"><img alt="../../../_images/OccupancyClassPred.png" src="../../../_images/OccupancyClassPred.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.4 </span><span class="caption-text">AI predicted occupancy types from street view images in Lake Charles.</span><a class="headerlink" href="#occ-class-pred" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="attribute-numberofstories">
<h4>Attribute: NumberOfStories<a class="headerlink" href="#attribute-numberofstories" title="Permalink to this headline"></a></h4>
<p>This attribute is determined by CityBuilder using an object detection procedure. A detection model that
can automatically detect rows of building windows was established to generate the image-based detections
of visible floor locations from street-level images. The model was trained on the
<a class="reference external" href="https://arxiv.org/abs/1911.09070">EfficientDet-D7 architecture</a> with a dataset of 60,000 images,
using 80% for training, 15% for validation, and 5% testing of the model. In order to ensure faster model
convergence, initial weights of the model were set to model weights of the (pretrained) object detection
model that, at the time, achieved state-of-the-art performance on the
<a class="reference external" href="https://cocodataset.org/#download">2017 COCO Detection set</a>. For this
specific implementation, the peak model performance was achieved using the <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam optimizer</a> at a learning
rate of 0.0001 (batch size: 2), after 50 epochs. <a class="reference internal" href="#num-stories-detection"><span class="std std-numref">Fig. 3.2.2.5</span></a> shows examples of the
floor detections performed by the model.</p>
<div class="align-center figure" id="num-stories-detection">
<a class="reference internal image-reference" href="../../../_images/NumOfStoriesDetection.png"><img alt="../../../_images/NumOfStoriesDetection.png" src="../../../_images/NumOfStoriesDetection.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.5 </span><span class="caption-text">Sample floor detections of the floor detection model (each detection is indicated by a green bounding box). The percentage value shown on the top right corner of a bounding box indicates model confidence level associated with that prediction.</span><a class="headerlink" href="#num-stories-detection" title="Permalink to this image"></a></p>
</div>
<p>For an image, the described floor detection model generates the bounding box output for its
detections and calculates the confidence level associated with each detection
(see <a class="reference internal" href="#num-stories-detection"><span class="std std-numref">Fig. 3.2.2.5</span></a>). A post-processor that converts stacks of neighboring
bounding boxes into floor counts was developed to convert this output into floor counts.
Recognizing an image may contain multiple buildings at a time, this post-processor was
designed to perform counts at the individual building level.</p>
<p>For a random image dataset of buildings captured using arbitrary camera orientations (also
termed in the wild images), the developed floor detection model was determined to capture
the number of floors information of buildings with an accuracy of 86%.
<a class="reference internal" href="#num-stories-vali"><span class="std std-numref">Fig. 3.2.2.6</span></a> (a) provides a breakdown of this accuracy measure for
different prediction classes (i.e. the confusion matrix of model classifications).
It was also observed that if the image dataset is established such that building images
are captured with minimal obstructions, the building is at the center of the image, and
perspective distortions are limited, the number of floors detections were performed at an
accuracy level of 94.7% by the model. <a class="reference internal" href="#num-stories-vali"><span class="std std-numref">Fig. 3.2.2.6</span></a> (b)
shows the confusion matrix for the model predicting on the “cleaned” image data.
In quantifying both accuracy levels, a test set of 3,000 images randomly selected
across all counties of a companion testbed in New Jersey, excluding Atlantic County (site of that
testbed), was utilized.</p>
<div class="align-center figure" id="num-stories-vali">
<a class="reference internal image-reference" href="../../../_images/NumOfStoriesVali.png"><img alt="../../../_images/NumOfStoriesVali.png" src="../../../_images/NumOfStoriesVali.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.6 </span><span class="caption-text">Confusion matrices for the number of floors predictor used in this study.</span><a class="headerlink" href="#num-stories-vali" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="attribute-meanroofht">
<span id="lbl-testbed-lc-asset-description-meanroofht"></span><h4>Attribute: MeanRoofHt<a class="headerlink" href="#attribute-meanroofht" title="Permalink to this headline"></a></h4>
<p>The elevation of the bottom plane of the roof (lowest edge of roof line) and elevation of the roof
(peak of gable or apex of hip) are estimated with respect to grade (in feet) from street-level imagery.
These geometric properties are defined visually for common residential coastal typologies in
<a class="reference internal" href="#building-elevation"><span class="std std-numref">Fig. 3.2.2.7</span></a>. The mean height of the roof system is then derived as
the average of these dimensions.</p>
<div class="align-center figure" id="building-elevation">
<a class="reference internal image-reference" href="../../../_images/BldgElev.png"><img alt="../../../_images/BldgElev.png" src="../../../_images/BldgElev.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.7 </span><span class="caption-text">Schematics demonstrating elevation quantities for different foundation systems common in coastal areas.</span><a class="headerlink" href="#building-elevation" title="Permalink to this image"></a></p>
</div>
<p>The MeanRoofHt is based on the following AI technique. <a class="reference internal" href="#mean-roof-ht-app"><span class="std std-numref">Fig. 3.2.2.8</span></a>
plots the predicted roof height versus the number of floors of the inventory.</p>
<p>As in any single-image metrology application, extracting the building elevations from imagery requires:</p>
<ol class="arabic simple">
<li><p>Rectification of image perspective distortions, typically introduced during capturing of an image capture.</p></li>
<li><p>Determining the pixel counts representing the distances between ends of the objects or surfaces of interest
(e.g., for first-floor height, the orthogonal distance between the ground and first-floor levels).</p></li>
<li><p>Converting these pixel counts to real-world dimensions by matching a reference measurement with the
corresponding pixel count.</p></li>
</ol>
<p>Given that the number of street-level images available for a building can be limited and sparsely spaced,
a single image rectification approach was deemed most applicable for regional-scale inventory
development. The first step in image rectification requires detecting line segments on the front
face of the building. This is performed by using the <a class="reference external" href="https://arxiv.org/abs/1905.03246">L-CNN</a>
end-to-end wireframe parsing method. Once the segments are detected, vertical and horizontal lines
on the front face of the building are automatically detected using
<a class="reference external" href="https://dl.acm.org/doi/10.1145/358669.358692">RANSAC</a> line fitting based on the
assumptions that line segments on this face are the predominant source of line segments in the image
and the orientation of these line segments change linearly with their horizontal or vertical position
depending on their predominant orientation. The Another support vector model implemented for image
rectification focuses on the street-facing plane of the building in an image, and, based on the
Manhattan World assumption, (i.e., all surfaces in the world are aligned with two horizontal and
one vertical dominant directions) iteratively transforms the image such that horizontal edges on the
facade plain lie parallel to each other, and its vertical edges are orthogonal to the horizontal edges.</p>
<p>In order to automate the process of obtaining the pixel counts for the ground elevations, a facade
segmentation model was trained to automatically label ground, facade, door, window, and roof pixels
in an image. The segmentation model was trained using
<a class="reference external" href="https://arxiv.org/abs/1706.05587">DeepLabV3 architecture on a ResNet-101 backbone</a>, pretrained on
<a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">PASCAL VOC 2012 segmentation dataset</a>, using a
facade segmentation dataset of 30,000 images supplemented with relevant portions of ADE20K segmentation
dataset. The peak model performance was attained using the <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam optimizer</a> at a learning rate of 0.001
(batch size: 4), after 40 epochs. The conversion between pixel dimensions and real-world dimensions were
attained by use of field of view and camera distance information collected for each street-level imagery.</p>
<p><a class="reference internal" href="#mean-roof-ht-app"><span class="std std-numref">Fig. 3.2.2.8</span></a> shows a scatter plot of the AI predicted mean roof heights vs AI-predicted number of floors.
A general trend observed in this plot is that the roof height increases with the number of floors,
which is in line with the general intuition.</p>
<div class="align-center figure" id="mean-roof-ht-app">
<a class="reference internal image-reference" href="../../../_images/MeanRoofHtApp.png"><img alt="../../../_images/MeanRoofHtApp.png" src="../../../_images/MeanRoofHtApp.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.8 </span><span class="caption-text">AI-predicted MeanRoofHt versus number of floors.</span><a class="headerlink" href="#mean-roof-ht-app" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="attribute-roofslope">
<h4>Attribute: RoofSlope<a class="headerlink" href="#attribute-roofslope" title="Permalink to this headline"></a></h4>
<p>RoofSlope is calculated as the ratio between the roof height and the roof run. Roof height is obtained
by determining the difference between the bottom plane and apex elevations of the roof as defined in the
<a class="reference internal" href="#lbl-testbed-lc-asset-description-meanroofht"><span class="std std-ref">Attribute: MeanRoofHt</span></a>
section. Roof run is determined as half the smaller dimension of the building, as determined from
the dimensions of the building footprint. <a class="reference internal" href="#mean-slope-app"><span class="std std-numref">Fig. 3.2.2.9</span></a> displays the AI-predicted mean roof height versus the
AI-precited roof pitch ratios. As expected, very little correlation between these two parameters are observed.</p>
<div class="align-center figure" id="mean-slope-app">
<a class="reference internal image-reference" href="../../../_images/RoofSlopeApp.png"><img alt="../../../_images/RoofSlopeApp.png" src="../../../_images/RoofSlopeApp.png" style="width: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.2.9 </span><span class="caption-text">AI-predicted RoofSlope versus mean roof height.</span><a class="headerlink" href="#mean-slope-app" title="Permalink to this image"></a></p>
</div>
</div>
</div>
</div>
<div class="section" id="phase-iii-augmentation-using-third-party-data-site-specific-observations-and-existing-knowledge">
<h2><span class="section-number">3.2.3. </span>Phase III: Augmentation Using Third-Party Data, Site-specific Observations, and Existing Knowledge<a class="headerlink" href="#phase-iii-augmentation-using-third-party-data-site-specific-observations-and-existing-knowledge" title="Permalink to this headline"></a></h2>
<p>The AI-generated building inventory is further augmented with multiple sources of information, including the
third-party datasets, site-specific statistics summarized from observations, and existing knowledge and
engineering judgement. The following attributes are obtained or derived from third-party data.</p>
<div class="section" id="attribute-dws-ii">
<h3>Attribute: DWS II<a class="headerlink" href="#attribute-dws-ii" title="Permalink to this headline"></a></h3>
<p>Design Wind Speed for Risk Category II construction in mph (ASCE 7-16), was obtained by queries to the
<a class="reference external" href="https://hazards.atcouncil.org/">ATC Hazards by Location API</a> (<a class="reference internal" href="#atc20" id="id7"><span>[ATC20]</span></a>).</p>
</div>
<div class="section" id="attribute-lulc">
<h3>Attribute: LULC<a class="headerlink" href="#attribute-lulc" title="Permalink to this headline"></a></h3>
<p>Land use code is downloaded from <a class="reference external" href="http://www.webgis.com/terr_pages/LA/lulcutm/calcasieu.html">WebGIS</a>.
Each land use class is represented by a integer as listed in <a class="reference internal" href="#tab-bldg-inv-data-model-lc"><span class="std std-numref">Table 3.2.1.1</span></a></p>
</div>
<div class="section" id="attribute-yearbuilt">
<h3>Attribute: YearBuilt<a class="headerlink" href="#attribute-yearbuilt" title="Permalink to this headline"></a></h3>
<p>We initially derived the year built information based on the National Structure Inventory (NSI), which contains year
built information for geocoded addresses in the region of interest. It should be noted that not all buildings
are included in the NSI dataset and the geocodes of the addresses do not match perfectly with building locations,
as shown in <a class="reference internal" href="#year-built-nsi"><span class="std std-numref">Fig. 3.2.3.1</span></a>.</p>
<div class="align-center figure" id="year-built-nsi">
<a class="reference internal image-reference" href="../../../_images/YearBuiltNSI.png"><img alt="../../../_images/YearBuiltNSI.png" src="../../../_images/YearBuiltNSI.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.3.1 </span><span class="caption-text">National Structure Inventory data points.</span><a class="headerlink" href="#year-built-nsi" title="Permalink to this image"></a></p>
</div>
<p>To address this issue, <a class="reference external" href="https://github.com/NHERI-SimCenter/SURF">SURF</a> (<a class="reference internal" href="#wang19" id="id9"><span>[Wang19]</span></a>) is employed to construct and train a neural
network on the year built information from
National Structure Inventory (NSI). The neural network is then used to predict the year built
information for each building based on the spatial patterns it learned from the NSI dataset.
The theory of using neural networks to learn the spatial patterns in data and to predict for
missing values is detailed <a class="reference external" href="https://doi.org/10.1016/j.autcon.2020.103474">here</a>.
The result is shown in <a class="reference internal" href="#year-built-comp"><span class="std std-numref">Fig. 3.2.3.2</span></a>.</p>
<div class="align-center figure" id="year-built-comp">
<a class="reference internal image-reference" href="../../../_images/YearBuiltComp.png"><img alt="../../../_images/YearBuiltComp.png" src="../../../_images/YearBuiltComp.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.3.2 </span><span class="caption-text">Comparison of year built between NSI and SURF.</span><a class="headerlink" href="#year-built-comp" title="Permalink to this image"></a></p>
</div>
<p>In parallel to this exploration, <a class="reference external" href="https://www.zillow.com/">Zillow</a> also provides the year built information for
many of the residual buildings in the studied region.</p>
<p>Similar to the implementation of NSI dataset, the 1182 data points of year built from Zillow are used to train a
neural network, <a class="reference internal" href="#surf-yb-test"><span class="std std-numref">Fig. 3.2.3.3</span></a> shows the verification of the trained neural network (predicted vs. true values,
Zillow dataset). More than <span class="math notranslate nohighlight">\(85%\)</span> buildings have prediction errors less than 20 years.</p>
<div class="align-center figure" id="surf-yb-test">
<a class="reference internal image-reference" href="../../../_images/SURF_YearBuiltTest.png"><img alt="../../../_images/SURF_YearBuiltTest.png" src="../../../_images/SURF_YearBuiltTest.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.3.3 </span><span class="caption-text">SURF-predicted vs. original year built from Zillow dataset.</span><a class="headerlink" href="#surf-yb-test" title="Permalink to this image"></a></p>
</div>
<p>The neural network is used to predict the year built information for the entire Lake Charles inventory. <a class="reference internal" href="#surf-yb-comp"><span class="std std-numref">Fig. 3.2.3.4</span></a>
contrast the resulting SURF-Zillow and the SURF-NSI year built spatial distribution. The difference in year built is relatively
small for the downtown buildings (~1960s) but increases at the bounds with a maximum of 80 years.
The Zillow-trained classifier is undergoing continued improvements and will be released with the next version of this testbed.
The current version of the testbed will thus use the NSI data as the basis for the Year Built Attribute</p>
<div class="align-center figure" id="surf-yb-comp">
<a class="reference internal image-reference" href="../../../_images/YearBuilt_NSI_SURFZS.png"><img alt="../../../_images/YearBuilt_NSI_SURFZS.png" src="../../../_images/YearBuilt_NSI_SURFZS.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.3.4 </span><span class="caption-text">SURF-NSI vs. SURF-Zillow: year built information.</span><a class="headerlink" href="#surf-yb-comp" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="attribute-garage">
<h3>Attribute: Garage<a class="headerlink" href="#attribute-garage" title="Permalink to this headline"></a></h3>
<p>A garage detector utilizing EfficienDet object detection architecture was trained to identify the
existence of attached garage and carport structures in street-level imagery of the buildings
included in the Lake Charles inventory. Properties are either classified as having an attached garage
or not having an attached garage (which includes both detached garages and homes with no garage)
The model was trained on the <a class="reference external" href="https://arxiv.org/abs/1911.09070">EfficientDet-D4 architecture</a> with
dataset of 1,887 images, using 80% for training, 10% for validation, and 10% for testing of the model.
Similar to the number of floors detector model, initial weights of this model were set to model weights
of the (pretrained) object detection model that, at the time, achieved state-of-the-art performance on
the <a class="reference external" href="https://cocodataset.org/#download">2017 COCO Detection set</a>. For this task, the peak detector
performance was attained using the <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam optimizer</a>
at a learning rate of 0.0001 (batch size: 2) after 25 epochs. <a class="reference internal" href="#garage-eg"><span class="std std-numref">Fig. 3.2.3.5</span></a> shows sample
garage detections performed by the model.</p>
<div class="align-center figure" id="garage-eg">
<a class="reference internal image-reference" href="../../../_images/GarageDetection.png"><img alt="../../../_images/GarageDetection.png" src="../../../_images/GarageDetection.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.3.5 </span><span class="caption-text">Samples of the garage detection model showing successful identification of attached garages and carports.</span><a class="headerlink" href="#garage-eg" title="Permalink to this image"></a></p>
</div>
<p>On the test set, the model achieves an accuracy of 92%. <a class="reference internal" href="#garage-cm"><span class="std std-numref">Fig. 3.2.3.6</span></a> shows the confusion matrix of the
model classifications on the test set. On a seperate test set consisting of images from only Lake Charles,
model performance is lower at 71%. <a class="reference internal" href="#garage-cm"><span class="std std-numref">Fig. 3.2.3.6</span></a> (b) shows the confusion matrix for model predictions on this
latter dataset.</p>
<div class="align-center figure" id="garage-cm">
<a class="reference internal image-reference" href="../../../_images/GarageConfusionMatrix.png"><img alt="../../../_images/GarageConfusionMatrix.png" src="../../../_images/GarageConfusionMatrix.png" style="width: 700px;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2.3.6 </span><span class="caption-text">Confusion matrices for the garage predictor used in this study. The matrix on the left (a) shows the model’s prediction accuracy when tested on a set of 189 images randomly selected from CA and NJ. The matrix on the right (b) depicts the model accuracy on images selected from the Lake Charles area.</span><a class="headerlink" href="#garage-cm" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="attribute-buildingtype">
<h3>Attribute: BuildingType<a class="headerlink" href="#attribute-buildingtype" title="Permalink to this headline"></a></h3>
<p>Based on information found in the National Structure Inventory, 89% of residential buildings
(single-family and multi-family) are wood, the rest are masonry. In the analysis, we conservatively
assume all residential buildings are wood.</p>
</div>
<div class="section" id="attribute-avgjantemp">
<h3>Attribute: AvgJanTemp<a class="headerlink" href="#attribute-avgjantemp" title="Permalink to this headline"></a></h3>
<p>The average temperature in Lake Charles in January is above the critical value of 25F,
based on NOAA average daily temperature. Referring <a class="reference internal" href="#tab-bldg-inv-data-model-lc"><span class="std std-numref">Table 3.2.1.1</span></a>, we used
“Above” for the buildings in the studied inventory.</p>
</div>
</div>
<div class="section" id="populated-inventories">
<h2><span class="section-number">3.2.4. </span>Populated Inventories<a class="headerlink" href="#populated-inventories" title="Permalink to this headline"></a></h2>
<p>Executing this three-phase process resulted in the assignment of all required attributes at the asset description
stage of the workflow for the Lake Charles building inventory, and <code class="xref std std-numref docutils literal notranslate"><span class="pre">bldg_inv_lc</span></code> shows example data samples.
The entire inventory can be accessed <a class="reference external" href="https://doi.org/10.17603/ds2-jpj2-zx14">here</a>.</p>
<dl class="citation">
<dt class="label" id="atc20"><span class="brackets"><a class="fn-backref" href="#id7">ATC20</a></span></dt>
<dd><p>ATC (2020b), ATC Hazards By Location, <a class="reference external" href="https://hazards.atcouncil.org/">https://hazards.atcouncil.org/</a>, Applied Technology Council, Redwood City, CA.</p>
</dd>
<dt class="label" id="wang19"><span class="brackets"><a class="fn-backref" href="#id9">Wang19</a></span></dt>
<dd><p>Wang C. (2019), NHERI-SimCenter/SURF: v0.2.0 (Version v0.2.0). Zenodo. <a class="reference external" href="http://doi.org/10.5281/zenodo.3463676">http://doi.org/10.5281/zenodo.3463676</a></p>
</dd>
<dt class="label" id="microsoft18"><span class="brackets"><a class="fn-backref" href="#id2">Microsoft18</a></span></dt>
<dd><p>Microsoft (2018), US Building Footprints. <a class="reference external" href="https://github.com/Microsoft/USBuildingFootprints">https://github.com/Microsoft/USBuildingFootprints</a></p>
</dd>
<dt class="label" id="fema21"><span class="brackets">FEMA21</span></dt>
<dd><p>FEMA (2021), Hazus Inventory Technical Manual. Hazus 4.2 Service Pack 3. Federal Emergency Management Agency, Washington D.C.</p>
</dd>
</dl>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, The Regents of the University of California.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=..."></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', '...', {
          'anonymize_ip': false,
      });
    </script>  

  <style>
         .wy-nav-content { max-width: none; }
  </style>

<script>
    /*
    let selectedFilters = [];
    const images  = document.getElementsByClassName("gallery-item");
    const filters = [...document.querySelectorAll('.filter select')];
    const toggles = [...document.querySelectorAll('.filter input')];

    var show = function (elem) {
        elem.style.display = 'block';
    };
    var hide = function (elem) {
        elem.style.display = 'none';
    };
    var toggleFilter =  function(el,elid) {
        const filter = document.getElementById(elid);
        filter.disabled = !el.checked;
       
        if ("createEvent" in document) {
            var evt = document.createEvent("HTMLEvents");
            evt.initEvent("change", false, true);
            filter.dispatchEvent(evt);
        }
        else
            filter.fireEvent("change");
    };
    
    for (const filter of filters) {
        filter.addEventListener('change', function(event) {
            selectedFilters = filters.map(filter => filter.disabled ? '' : filter.value).filter(Boolean);
            console.log(selectedFilters);
            for (const image of images) {
                if (selectedFilters.every(filter => image.classList.contains(filter))) {
                    show(image);
                }
                else {hide(image)};
            };
        })
    };
    */
</script>


</body>
</html>